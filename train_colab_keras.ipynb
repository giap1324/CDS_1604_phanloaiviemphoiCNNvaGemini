{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93767e73",
   "metadata": {},
   "source": [
    "### 1. Mount Google Drive v√† Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29971357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9257b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install tensorflow pillow scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9183b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eec26bb",
   "metadata": {},
   "source": [
    "### 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50affb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ƒê∆Ø·ªúNG D·∫™N S·∫º ƒê∆Ø·ª¢C C·∫¨P NH·∫¨T ·ªû CELL TR√äN\n",
    "# Sau khi ch·∫°y c√°c cell ki·ªÉm tra, quay l·∫°i ƒë√¢y ƒë·ªÉ x√°c nh·∫≠n\n",
    "\n",
    "# Data directories\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data', 'processed')\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, 'train')\n",
    "VAL_DIR = os.path.join(DATA_DIR, 'val')\n",
    "TEST_DIR = os.path.join(DATA_DIR, 'test')\n",
    "\n",
    "# Model directory\n",
    "MODEL_DIR = os.path.join(BASE_DIR, 'models')\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Hyperparameters\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.0001\n",
    "EPOCHS = 20\n",
    "NUM_CLASSES = 2\n",
    "DROPOUT_RATE = 0.5\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Data directory: {DATA_DIR}\")\n",
    "print(f\"  Model directory: {MODEL_DIR}\")\n",
    "print(f\"  Image size: {IMG_SIZE}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "# Ki·ªÉm tra c√°c th∆∞ m·ª•c\n",
    "print(\"\\nChecking directories:\")\n",
    "for name, path in [('TRAIN', TRAIN_DIR), ('VAL', VAL_DIR), ('TEST', TEST_DIR)]:\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            subdirs = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n",
    "            print(f\"  ‚úì {name}: {path}\")\n",
    "            print(f\"    Classes: {subdirs}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚úó {name}: Error - {e}\")\n",
    "    else:\n",
    "        print(f\"  ‚úó {name}: NOT FOUND - {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556941ae",
   "metadata": {},
   "source": [
    "### 2.1. Ki·ªÉm tra v√† t√¨m th∆∞ m·ª•c d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a24161a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ki·ªÉm tra c·∫•u tr√∫c th∆∞ m·ª•c trong Google Drive\n",
    "import os\n",
    "\n",
    "# Li·ªát k√™ c√°c th∆∞ m·ª•c trong MyDrive\n",
    "print(\"Checking Google Drive structure...\")\n",
    "drive_path = '/content/drive/MyDrive'\n",
    "\n",
    "if os.path.exists(drive_path):\n",
    "    print(f\"\\n‚úì Found: {drive_path}\")\n",
    "    print(\"\\nFolders in MyDrive:\")\n",
    "    for item in os.listdir(drive_path):\n",
    "        item_path = os.path.join(drive_path, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            print(f\"  üìÅ {item}\")\n",
    "    \n",
    "    # T√¨m th∆∞ m·ª•c ch·ª©a data\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Searching for data folders...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # T√¨m ki·∫øm c√°c th∆∞ m·ª•c c√≥ t√™n li√™n quan\n",
    "    possible_folders = []\n",
    "    for item in os.listdir(drive_path):\n",
    "        item_path = os.path.join(drive_path, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            item_lower = item.lower()\n",
    "            if any(keyword in item_lower for keyword in ['pneumonia', 'chest', 'xray', 'data', 'covid']):\n",
    "                possible_folders.append(item)\n",
    "                print(f\"‚úì Found potential data folder: {item}\")\n",
    "                \n",
    "                # Ki·ªÉm tra b√™n trong\n",
    "                try:\n",
    "                    sub_items = os.listdir(item_path)\n",
    "                    if 'data' in sub_items or 'chest_xray' in sub_items:\n",
    "                        print(f\"  ‚Üí Contains: {', '.join(sub_items[:5])}\")\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    if not possible_folders:\n",
    "        print(\"\\n‚ö† No data folders found!\")\n",
    "        print(\"Please upload your data to Google Drive first.\")\n",
    "        print(\"\\nExpected structure:\")\n",
    "        print(\"  MyDrive/\")\n",
    "        print(\"    ‚îî‚îÄ‚îÄ pneumonia_project/  (or your project folder)\")\n",
    "        print(\"        ‚îî‚îÄ‚îÄ data/\")\n",
    "        print(\"            ‚îî‚îÄ‚îÄ processed/\")\n",
    "        print(\"                ‚îú‚îÄ‚îÄ train/\")\n",
    "        print(\"                ‚îÇ   ‚îú‚îÄ‚îÄ NORMAL/\")\n",
    "        print(\"                ‚îÇ   ‚îî‚îÄ‚îÄ PNEUMONIA/\")\n",
    "        print(\"                ‚îú‚îÄ‚îÄ val/\")\n",
    "        print(\"                ‚îî‚îÄ‚îÄ test/\")\n",
    "else:\n",
    "    print(f\"‚úó Drive not mounted at {drive_path}\")\n",
    "    print(\"Please run the Mount Drive cell first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc1825a",
   "metadata": {},
   "source": [
    "### 2.2. C·∫≠p nh·∫≠t ƒë∆∞·ªùng d·∫´n (Ch·∫°y sau khi ki·ªÉm tra xong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af4e65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C·∫¨P NH·∫¨T ƒê∆Ø·ªúNG D·∫™N N√ÄY D·ª±A tr√™n k·∫øt qu·∫£ ki·ªÉm tra ·ªü tr√™n\n",
    "# V√≠ d·ª• c√°c tr∆∞·ªùng h·ª£p ph·ªï bi·∫øn:\n",
    "\n",
    "# Tr∆∞·ªùng h·ª£p 1: D·ªØ li·ªáu trong th∆∞ m·ª•c ri√™ng\n",
    "BASE_DIR = '/content/drive/MyDrive/pneumonia_project'\n",
    "\n",
    "# Tr∆∞·ªùng h·ª£p 2: D·ªØ li·ªáu trong th∆∞ m·ª•c COVID-19_Radiography_Dataset\n",
    "# BASE_DIR = '/content/drive/MyDrive/COVID-19_Radiography_Dataset'\n",
    "\n",
    "# Tr∆∞·ªùng h·ª£p 3: D·ªØ li·ªáu trong th∆∞ m·ª•c chest_xray\n",
    "# BASE_DIR = '/content/drive/MyDrive/chest_xray'\n",
    "\n",
    "# Tr∆∞·ªùng h·ª£p 4: N·∫øu ƒë√£ c√≥ data/processed s·∫µn trong m·ªôt th∆∞ m·ª•c kh√°c\n",
    "# BASE_DIR = '/content/drive/MyDrive/your_folder_name'\n",
    "\n",
    "print(f\"Using BASE_DIR: {BASE_DIR}\")\n",
    "\n",
    "# Ki·ªÉm tra xem th∆∞ m·ª•c c√≥ t·ªìn t·∫°i kh√¥ng\n",
    "if os.path.exists(BASE_DIR):\n",
    "    print(\"‚úì BASE_DIR exists!\")\n",
    "    print(f\"\\nContents of {BASE_DIR}:\")\n",
    "    try:\n",
    "        for item in os.listdir(BASE_DIR)[:10]:  # Ch·ªâ hi·ªÉn th·ªã 10 items ƒë·∫ßu\n",
    "            print(f\"  - {item}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "else:\n",
    "    print(f\"‚úó BASE_DIR does not exist: {BASE_DIR}\")\n",
    "    print(\"\\n‚ö† PLEASE UPDATE BASE_DIR ABOVE!\")\n",
    "    print(\"Run the previous cell to see available folders.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3c1c3a",
   "metadata": {},
   "source": [
    "### 3. Data Generators v·ªõi Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f049fe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation cho training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    zoom_range=0.1,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Kh√¥ng augmentation cho validation v√† test\n",
    "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "print(\"Data generators created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb5c728",
   "metadata": {},
   "source": [
    "### 4. Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7e5b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Validation data\n",
    "val_generator = val_test_datagen.flow_from_directory(\n",
    "    VAL_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Test data\n",
    "test_generator = val_test_datagen.flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"\\nClass indices: {train_generator.class_indices}\")\n",
    "print(f\"Total training samples: {train_generator.samples}\")\n",
    "print(f\"Total validation samples: {val_generator.samples}\")\n",
    "print(f\"Total test samples: {test_generator.samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a27e49",
   "metadata": {},
   "source": [
    "### 5. Build Model v·ªõi DenseNet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b84bf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(img_size=IMG_SIZE, num_classes=NUM_CLASSES, dropout_rate=DROPOUT_RATE):\n",
    "    \"\"\"T·∫°o model DenseNet121 v·ªõi transfer learning\"\"\"\n",
    "    \n",
    "    # Load DenseNet121 pretrained tr√™n ImageNet\n",
    "    base_model = DenseNet121(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(img_size[0], img_size[1], 3)\n",
    "    )\n",
    "    \n",
    "    # Freeze base model layers\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Build model\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# T·∫°o model\n",
    "model = create_model()\n",
    "model.summary()\n",
    "\n",
    "print(f\"\\nTotal parameters: {model.count_params():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d54195",
   "metadata": {},
   "source": [
    "### 6. Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe4f5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Model compiled successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e31dfee",
   "metadata": {},
   "source": [
    "### 7. Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee55de75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model checkpoint - l∆∞u model t·ªët nh·∫•t\n",
    "checkpoint_path = os.path.join(MODEL_DIR, 'best_model.h5')\n",
    "checkpoint = ModelCheckpoint(\n",
    "    checkpoint_path,\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Reduce learning rate khi validation loss kh√¥ng gi·∫£m\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Early stopping\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callbacks = [checkpoint, reduce_lr, early_stop]\n",
    "print(\"Callbacks configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbddb6d",
   "metadata": {},
   "source": [
    "### 8. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6c5062",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Training\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETED!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c89564",
   "metadata": {},
   "source": [
    "### 9. Visualization - Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930dd84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V·∫Ω bi·ªÉu ƒë·ªì training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss plot\n",
    "ax1.plot(history.history['loss'], label='Train Loss', marker='o')\n",
    "ax1.plot(history.history['val_loss'], label='Val Loss', marker='s')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy plot\n",
    "ax2.plot(history.history['accuracy'], label='Train Accuracy', marker='o')\n",
    "ax2.plot(history.history['val_accuracy'], label='Val Accuracy', marker='s')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Training and Validation Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "history_path = os.path.join(MODEL_DIR, 'training_history.png')\n",
    "plt.savefig(history_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training history saved to {history_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69da53bc",
   "metadata": {},
   "source": [
    "### 10. Test Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10acc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EVALUATING ON TEST SET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load model t·ªët nh·∫•t\n",
    "best_model = keras.models.load_model(checkpoint_path)\n",
    "\n",
    "# Evaluate tr√™n test set\n",
    "test_loss, test_acc = best_model.evaluate(test_generator, verbose=1)\n",
    "\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"  Loss: {test_loss:.4f}\")\n",
    "print(f\"  Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af2eb29",
   "metadata": {},
   "source": [
    "### 11. Predictions v√† Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d6b950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "test_generator.reset()\n",
    "predictions = best_model.predict(test_generator, verbose=1)\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "y_true = test_generator.classes\n",
    "\n",
    "# Class names\n",
    "class_names = list(test_generator.class_indices.keys())\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18a9692",
   "metadata": {},
   "source": [
    "### 12. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46129bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V·∫Ω confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "\n",
    "cm_path = os.path.join(MODEL_DIR, 'confusion_matrix.png')\n",
    "plt.savefig(cm_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Confusion matrix saved to {cm_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c00985",
   "metadata": {},
   "source": [
    "### 13. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ffdad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L∆∞u model cu·ªëi c√πng\n",
    "final_model_path = os.path.join(MODEL_DIR, 'final_model.h5')\n",
    "best_model.save(final_model_path)\n",
    "\n",
    "print(f\"\\nFinal model saved to {final_model_path}\")\n",
    "print(f\"Model format: HDF5 (.h5)\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL DONE!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9675d8e8",
   "metadata": {},
   "source": [
    "### 14. Model Summary v√† Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605221de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hi·ªÉn th·ªã th√¥ng tin model\n",
    "print(\"Model Information:\")\n",
    "print(f\"  Architecture: DenseNet121\")\n",
    "print(f\"  Input shape: {IMG_SIZE + (3,)}\")\n",
    "print(f\"  Number of classes: {NUM_CLASSES}\")\n",
    "print(f\"  Class names: {class_names}\")\n",
    "print(f\"  Test accuracy: {test_acc:.4f}\")\n",
    "print(f\"\\nModel files saved:\")\n",
    "print(f\"  - {checkpoint_path}\")\n",
    "print(f\"  - {final_model_path}\")\n",
    "print(f\"  - {history_path}\")\n",
    "print(f\"  - {cm_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be72ccb4",
   "metadata": {},
   "source": [
    "### 15. Download Models (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1398dfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model v·ªÅ local machine\n",
    "from google.colab import files\n",
    "\n",
    "# Download best model\n",
    "files.download(checkpoint_path)\n",
    "\n",
    "# Download final model\n",
    "files.download(final_model_path)\n",
    "\n",
    "# Download plots\n",
    "files.download(history_path)\n",
    "files.download(cm_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509dfd25",
   "metadata": {},
   "source": [
    "### 16. How to Load Model Later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c837779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√°ch load model ƒë·ªÉ s·ª≠ d·ª•ng sau n√†y\n",
    "# loaded_model = keras.models.load_model(final_model_path)\n",
    "# predictions = loaded_model.predict(your_data)\n",
    "\n",
    "print(\"To load model later, use:\")\n",
    "print(\"  model = keras.models.load_model('final_model.h5')\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
